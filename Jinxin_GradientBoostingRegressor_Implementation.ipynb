{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor Implementation\n",
    "\n",
    "* In this notebook, I implemented the gradient boosted trees to solve for a regression problem\n",
    "* Then I verified the algorithm by comparing the results between Scikit-learn and my implementation. Note that values of the paramters (e.g. max_depth, n_estimators) used in both implementations are the same\n",
    "* I referred to the pesudo code from this paper: https://statweb.stanford.edu/~jhf/ftp/trebst.pdf (Page 5-7) as well as Wikipedia: https://en.wikipedia.org/wiki/Gradient_boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. GradientBoostingRegressor from Scikit-learn, using the friedman data: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using LS loss function: 0.235100371411\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, min_samples_leaf=1, \\\n",
    "                                max_depth=3, random_state=0, loss='ls').fit(X_train, y_train)\n",
    "print \"MSE using LS loss function: \" + str(mean_squared_error(y_train, est.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. My GBRegressor implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_pseudo_response(y_train, y_pred):\n",
    "    return y_train - y_pred\n",
    "\n",
    "def loss_function(loss):\n",
    "    loss_dict = {'ls': least_squares_pseudo_response} # can add alternative loss function in\n",
    "                                                      # the future\n",
    "    return loss_dict[loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GBRegressor():\n",
    "    def __init__(self, n_estimators=10, min_sample=1, max_depth=3, \\\n",
    "                 learning_rate=0.1, loss_metric='ls'):\n",
    "        '''\n",
    "        Initialize the Gradient Boosting Regressor\n",
    "        @param n_estimators: number of estimators\n",
    "        @param min_sample: The minimum number of samples required in each leaf\n",
    "        @param max_depth: maximum depth of the individual regression estimators\n",
    "        @param learning_rate: learning rate shrinks the contribution of each tree\n",
    "        @param loss_metric: loss function to be optimized\n",
    "        '''\n",
    "        self.n_estimators = n_estimators\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_metric = loss_metric\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        '''\n",
    "        Fit gradient boosting regressor\n",
    "        '''\n",
    "        y_pred = np.zeros(len(y_train))\n",
    "        pseudo_response = loss_function(self.loss_metric)(y_train, y_pred) \n",
    "        tree_dict = dict()\n",
    "        \n",
    "        for i in xrange(self.n_estimators):           \n",
    "            algo = DecisionTreeRegressor(min_samples_leaf=self.min_sample, max_depth=self.max_depth, \\\n",
    "                                         criterion='mse', random_state=0)\n",
    "            algo.fit(x_train, pseudo_response)\n",
    "            tree_dict[i] = algo\n",
    "            resid_pred = algo.predict(x_train)\n",
    "            y_pred += self.learning_rate * resid_pred\n",
    "            pseudo_response = loss_function(self.loss_metric)(y_train, y_pred)        \n",
    "        self.tree_dict = tree_dict\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        '''\n",
    "        Make predictions\n",
    "        '''\n",
    "        y_pred_test = np.zeros(len(x_test))\n",
    "        for i in xrange(self.n_estimators):\n",
    "            tree_stump_pred = self.tree_dict[i].predict(x_test)\n",
    "            y_pred_test += self.learning_rate * tree_stump_pred\n",
    "        return y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using LS loss function: 0.235100511898\n"
     ]
    }
   ],
   "source": [
    "algo = GBRegressor(n_estimators=100, learning_rate=0.1, min_sample=1, \\\n",
    "                   max_depth=3, loss_metric='ls')\n",
    "algo.fit(X_train, y_train)\n",
    "print \"MSE using LS loss function: \" + str(mean_squared_error(y_train, algo.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. Conclusion\n",
    "* The MSE from GradientBoostingRegressor is 0.235100371411\n",
    "* The MSE from GBRegressor is 0.235100511898. The results are almost the same\n",
    "* Technically, each tree in gradient boosted trees should have a different weight, but it turned out using a fixed weight for each tree (in my implementation, the weight is 1) works fine.\n",
    "* Future improvement can include: \n",
    "   1. User can use alternative loss functions e.g. Least-abosulote-deviation loss function and Huber loss function\n",
    "   2. The implementation of GradientBoostingClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
